"""Streamlitãƒ™ãƒ¼ã‚¹ã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯åˆ†æã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³"""

from plotly.graph_objs._figure import Figure
import streamlit as st
import polars as pl
import tempfile
from typing import Optional, List, Dict, Any, Sequence
from pathlib import Path

from streamlit.delta_generator import DeltaGenerator
from streamlit.runtime.uploaded_file_manager import UploadedFile

from bookmark_analytics_toolkit.i18n import I18n
from src.bookmark_analytics_toolkit.data import BookmarkLoader, BookmarkPreprocessor
from src.bookmark_analytics_toolkit.analysis import (
    StatisticsAnalyzer,
    TimeSeriesAnalyzer,
    HierarchyAnalyzer,
)

# SudachiPyç‰ˆã‚’ä½¿ç”¨
from src.bookmark_analytics_toolkit.analysis.text_analysis import TextAnalyzer
from src.bookmark_analytics_toolkit.visualization import PlotlyCharts
from src.bookmark_analytics_toolkit.visualization.wordcloud_viz import WordCloudGenerator
from src.bookmark_analytics_toolkit.i18n import get_i18n

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="Bookmark Analytics Toolkit",
    page_icon="ğŸ“š",
    layout="wide",
    initial_sidebar_state="expanded",
)

# ã‚¿ãƒ–ã‚’æ”¹è¡Œå¯èƒ½ã«ã™ã‚‹ã‚«ã‚¹ã‚¿ãƒ CSS
st.markdown(
    """
<style>
    /* ã‚¿ãƒ–ã‚³ãƒ³ãƒ†ãƒŠã‚’æ”¹è¡Œå¯èƒ½ã«ã™ã‚‹ */
    .stTabs [data-baseweb="tab-list"] {
        flex-wrap: wrap !important;
        gap: 8px;
        row-gap: 8px;
    }

    /* å€‹åˆ¥ã®ã‚¿ãƒ–ãƒœã‚¿ãƒ³ */
    .stTabs [data-baseweb="tab"] {
        flex-shrink: 0;
        white-space: nowrap;
        margin-right: 4px;
    }
</style>
""",
    unsafe_allow_html=True,
)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if "df" not in st.session_state:
    st.session_state.df = None
if "preprocessed_df" not in st.session_state:
    st.session_state.preprocessed_df = None
if "language" not in st.session_state:
    st.session_state.language = "ja"
if "word_analysis_cache" not in st.session_state:
    st.session_state.word_analysis_cache = None  # å¹´æ¯ã®å˜èªå‡ºç¾é »åº¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥
if "sudachi_mode" not in st.session_state:
    st.session_state.sudachi_mode = "C"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ãƒ¢ãƒ¼ãƒ‰C
if "applied_use_all_years" not in st.session_state:
    st.session_state.applied_use_all_years = True  # é©ç”¨æ¸ˆã¿ã®å…¨æœŸé–“ãƒ•ãƒ©ã‚°
if "applied_selected_years" not in st.session_state:
    st.session_state.applied_selected_years = []  # é©ç”¨æ¸ˆã¿ã®é¸æŠã•ã‚ŒãŸå¹´
if "uploaded_filename" not in st.session_state:
    st.session_state.uploaded_filename = None  # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«å

# å¤šè¨€èªå¯¾å¿œ
i18n: I18n = get_i18n()
i18n.set_language(st.session_state.language)


def load_data(uploaded_file) -> None:
    """ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
    try:
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆWindows/Linux/Macäº’æ›ï¼‰
        with tempfile.NamedTemporaryFile(
            mode="wb", delete=False, suffix=Path(uploaded_file.name).suffix
        ) as tmp_file:
            tmp_file.write(uploaded_file.getbuffer())
            temp_path = Path(tmp_file.name)

        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        df: pl.DataFrame = BookmarkLoader.load(temp_path)
        BookmarkLoader.validate_schema(df)

        # å‰å‡¦ç†
        preprocessed_df: pl.DataFrame = BookmarkPreprocessor.preprocess(df)

        # ãƒ†ã‚­ã‚¹ãƒˆåˆ†æã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆï¼ˆå¹´æ¯ã«åˆ†æï¼‰
        with st.spinner(i18n.get("analyzing_text")):
            text_analyzer = TextAnalyzer(mode=st.session_state.sudachi_mode)
            word_analysis_cache = _build_word_analysis_cache(preprocessed_df, text_analyzer)

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜
        st.session_state.df = df
        st.session_state.preprocessed_df = preprocessed_df
        st.session_state.word_analysis_cache = word_analysis_cache
        st.session_state.uploaded_filename = uploaded_file.name  # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ä¿å­˜

        # å¹´ãƒ•ã‚£ãƒ«ã‚¿ã‚’ãƒªã‚»ãƒƒãƒˆï¼ˆå…¨æœŸé–“ã«æˆ»ã™ï¼‰
        st.session_state.applied_use_all_years = True
        st.session_state.applied_selected_years = []

        st.success(i18n.get("load_success", count=len(df)))

    except Exception as e:
        st.error(i18n.get("load_error", error=str(e)))


def _build_word_analysis_cache(df: pl.DataFrame, text_analyzer: TextAnalyzer) -> Dict[str, Any]:
    """
    å¹´æ¯ã«å½¢æ…‹ç´ è§£æã‚’å®Ÿè¡Œã—ã€å˜èªå‡ºç¾é »åº¦ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥

    Returns:
        {
            'all': {'å˜èª': å‡ºç¾å›æ•°, ...},  # å…¨æœŸé–“
            2023: {'å˜èª': å‡ºç¾å›æ•°, ...},    # 2023å¹´
            2024: {'å˜èª': å‡ºç¾å›æ•°, ...},    # 2024å¹´
            ...
        }
    """
    cache: Dict[str, Dict[str, int]] = {}

    # å…¨æœŸé–“ã®åˆ†æ
    word_freq_all = text_analyzer.get_word_frequency(df, title_column="Title")
    cache["all"] = word_freq_all

    # å¹´æ¯ã®åˆ†æ
    if "created_year" in df.columns:
        years = df["created_year"].unique().sort().to_list()
        for year in years:
            if year is not None:
                year_df = df.filter(pl.col("created_year") == year)
                word_freq_year = text_analyzer.get_word_frequency(year_df, title_column="Title")
                cache[int(year)] = word_freq_year

    return cache


def _get_word_freq_from_cache(
    cache: Dict[str, Dict[str, int]], use_all_years: bool, selected_years: List[int]
) -> Dict[str, int]:
    """
    ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰è©²å½“ã™ã‚‹å¹´ã®å˜èªé »åº¦ã‚’å–å¾—

    Args:
        cache: å˜èªé »åº¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        use_all_years: å…¨æœŸé–“ã‚’ä½¿ç”¨ã™ã‚‹ã‹
        selected_years: é¸æŠã•ã‚ŒãŸå¹´ã®ãƒªã‚¹ãƒˆ

    Returns:
        å˜èªé »åº¦ã®è¾æ›¸
    """
    if use_all_years:
        # å…¨æœŸé–“
        return cache.get("all", {})
    elif not selected_years:
        # å¹´ãŒé¸æŠã•ã‚Œã¦ã„ãªã„å ´åˆã¯ç©º
        return {}
    elif len(selected_years) == 1:
        # å˜ä¸€å¹´ã®å ´åˆ
        return cache.get(selected_years[0], {})
    else:
        # è¤‡æ•°å¹´ã®å ´åˆã¯ã€å„å¹´ã®é »åº¦ã‚’ãƒãƒ¼ã‚¸
        merged_freq: Dict[str, int] = {}
        for year in selected_years:
            year_freq = cache.get(year, {})
            for word, count in year_freq.items():
                merged_freq[word] = merged_freq.get(word, 0) + count
        return merged_freq


# ã‚µã‚¤ãƒ‰ãƒãƒ¼
with st.sidebar:
    st.title(i18n.get("app_title"))

    # è¨€èªé¸æŠ
    language_option: str = st.selectbox(
        i18n.get("language"),
        options=["æ—¥æœ¬èª", "English"],
        index=0 if st.session_state.language == "ja" else 1,
    )

    if language_option == "æ—¥æœ¬èª" and st.session_state.language != "ja":
        st.session_state.language = "ja"
        i18n.set_language("ja")
        st.rerun()
    elif language_option == "English" and st.session_state.language != "en":
        st.session_state.language = "en"
        i18n.set_language("en")
        st.rerun()

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    uploaded_file: UploadedFile | None = st.file_uploader(
        i18n.get("load_file"),
        type=["json", "csv"],
    )

    if uploaded_file is not None:
        # æ—¢ã«åŒã˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if st.session_state.uploaded_filename != uploaded_file.name:
            load_data(uploaded_file)

    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«åã‚’è¡¨ç¤ºï¼ˆè¨€èªåˆ‡æ›¿å¾Œã‚‚è¡¨ç¤ºã‚’ç¶­æŒï¼‰
    if st.session_state.uploaded_filename is not None:
        st.caption(f"ğŸ“„ {st.session_state.uploaded_filename}")

    # Sudachiãƒ¢ãƒ¼ãƒ‰é¸æŠ
    if st.session_state.preprocessed_df is not None:
        st.markdown("---")
        st.subheader(i18n.get("morphological_analysis_settings"))

        # ãƒ¢ãƒ¼ãƒ‰é¸æŠã®èª¬æ˜
        with st.expander(i18n.get("sudachi_mode_about"), expanded=False):
            st.markdown(i18n.get("sudachi_mode_description"))

        # ãƒ¢ãƒ¼ãƒ‰ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’å¤šè¨€èªåŒ–
        mode_options = [
            i18n.get("sudachi_mode_a"),
            i18n.get("sudachi_mode_b"),
            i18n.get("sudachi_mode_c"),
        ]

        sudachi_mode_option = st.radio(
            i18n.get("sudachi_mode_label"),
            options=mode_options,
            index=["A", "B", "C"].index(st.session_state.sudachi_mode),
            horizontal=True,
            key="sudachi_mode_radio",
        )

        # ãƒ¢ãƒ¼ãƒ‰ã®å¤‰æ›´ã‚’æ¤œå‡º
        # é¸æŠã•ã‚ŒãŸã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‹ã‚‰ãƒ¢ãƒ¼ãƒ‰æ–‡å­—ã‚’æŠ½å‡º
        mode_index = mode_options.index(sudachi_mode_option)
        new_mode = ["A", "B", "C"][mode_index]

        if new_mode != st.session_state.sudachi_mode:
            st.session_state.sudachi_mode = new_mode
            # ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯å†è§£æ
            if st.session_state.preprocessed_df is not None:
                with st.spinner(i18n.get("sudachi_mode_changing")):
                    text_analyzer = TextAnalyzer(mode=new_mode)
                    st.session_state.word_analysis_cache = _build_word_analysis_cache(
                        st.session_state.preprocessed_df, text_analyzer
                    )
                st.success(i18n.get("sudachi_mode_changed", mode=new_mode))
                st.rerun()

    # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¹´ãƒ•ã‚£ãƒ«ã‚¿
    if st.session_state.preprocessed_df is not None:
        st.markdown("---")
        st.subheader(i18n.get("year_filter_label"))

        # åˆ©ç”¨å¯èƒ½ãªå¹´ã‚’å–å¾—
        available_years: List[int] = TimeSeriesAnalyzer.get_available_years(
            st.session_state.preprocessed_df
        )

        # å…¨æœŸé–“ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹
        use_all_years = st.checkbox(
            i18n.get("all_years"),
            value=st.session_state.applied_use_all_years,
            key="all_years_cb",
        )

        # ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã®çŠ¶æ…‹å¤‰åŒ–ã‚’æ¤œå‡º
        if use_all_years != st.session_state.applied_use_all_years:
            if use_all_years:
                # å…¨æœŸé–“ONã®å ´åˆ
                st.session_state.applied_use_all_years = True
                st.session_state.applied_selected_years = []
            else:
                # å…¨æœŸé–“OFFã®å ´åˆã€æœ€æ–°å¹´ã‚’è‡ªå‹•é¸æŠ
                st.session_state.applied_use_all_years = False
                if available_years:
                    latest_year = max(available_years)
                    st.session_state.applied_selected_years = [latest_year]
            st.rerun()

        # å¹´é¸æŠãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã®è¡¨ç¤º
        if not st.session_state.applied_use_all_years:
            st.write("å¹´ã‚’é¸æŠ:")

            # ä¸€è¡Œã«3å€‹ãšã¤ä¸¦ã¹ã‚‹ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ
            for i in range(0, len(available_years), 3):
                cols = st.columns(3)
                for j, year in enumerate(available_years[i:i+3]):
                    with cols[j]:
                        # ã“ã®å¹´ãŒé¸æŠã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                        is_selected = year in st.session_state.applied_selected_years
                        st.checkbox(
                            str(year),
                            value=is_selected,
                            key=f"year_cb_{year}",
                        )

            # å¹´é¸æŠã®å¤‰åŒ–ã‚’æ¤œå‡º
            selected = []
            for year in available_years:
                year_key = f"year_cb_{year}"
                if st.session_state.get(year_key, False):
                    selected.append(year)

            # é¸æŠçŠ¶æ…‹ãŒå¤‰åŒ–ã—ãŸå ´åˆ
            if selected != st.session_state.applied_selected_years:
                if selected:
                    # å¹´ãŒé¸æŠã•ã‚Œã¦ã„ã‚‹å ´åˆ
                    st.session_state.applied_use_all_years = False
                    st.session_state.applied_selected_years = selected
                else:
                    # ä½•ã‚‚é¸æŠã•ã‚Œã¦ã„ãªã„å ´åˆã¯å…¨æœŸé–“ã«æˆ»ã™
                    st.session_state.applied_use_all_years = True
                    st.session_state.applied_selected_years = []
                st.rerun()
        else:
            # å…¨æœŸé–“é¸æŠæ™‚ã®ãƒ˜ãƒ«ãƒ—ãƒ†ã‚­ã‚¹ãƒˆ
            st.caption("ğŸ’¡ ä¸Šã®ãƒã‚§ãƒƒã‚¯ã‚’å¤–ã™ã¨ã€ç‰¹å®šã®å¹´ã‚’é¸æŠã—ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã§ãã¾ã™")

    # çµ±è¨ˆæƒ…å ±è¡¨ç¤º
    if st.session_state.preprocessed_df is not None:
        # ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨å¾Œã®ãƒ‡ãƒ¼ã‚¿ã§çµ±è¨ˆã‚’è¨ˆç®—
        filtered_df = st.session_state.preprocessed_df
        if not st.session_state.applied_use_all_years and st.session_state.applied_selected_years:
            filtered_df = filtered_df.filter(
                pl.col("created_year").is_in(st.session_state.applied_selected_years)
            )

        stats: Dict[str, Any] = StatisticsAnalyzer.get_basic_stats(filtered_df)
        st.markdown("---")
        st.metric(i18n.get("bookmarks"), stats["total_bookmarks"])
        st.metric(i18n.get("folders"), stats["total_folders"])
        st.metric(i18n.get("avg_depth"), f"{stats['avg_hierarchy_depth']:.1f}")

# ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢
if st.session_state.preprocessed_df is None:
    st.info(i18n.get("welcome_message"))
else:
    # ã‚¿ãƒ–ä½œæˆ
    tabs: Sequence[DeltaGenerator] = st.tabs(
        [
            i18n.get("browser_dist"),
            i18n.get("folder_dist"),
            i18n.get("domain_dist"),
            i18n.get("monthly_trend"),
            i18n.get("yearly_trend"),
            i18n.get("weekday_pattern"),
            i18n.get("hour_pattern"),
            i18n.get("weekday_hour_heatmap"),
            i18n.get("hierarchy_treemap"),
            i18n.get("folder_tree"),
            i18n.get("wordcloud"),
            i18n.get("word_ranking"),
        ]
    )

    # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¹´ãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨ï¼ˆé©ç”¨æ¸ˆã¿ã®ãƒ•ã‚£ãƒ«ã‚¿ã‚’ä½¿ç”¨ï¼‰
    df = st.session_state.preprocessed_df
    use_all_years = st.session_state.applied_use_all_years
    selected_years: List[int] = st.session_state.applied_selected_years

    if not use_all_years:
        # å¹´ãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨
        if selected_years:
            df = df.filter(pl.col("created_year").is_in(selected_years))

    # ã‚¿ã‚¤ãƒˆãƒ«ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ
    title_suffix = ""
    if not use_all_years and selected_years:
        year_str: str = "ã€".join([f"{y}å¹´" for y in sorted(selected_years)])
        title_suffix: str = f": {year_str}"

    # ãƒ–ãƒ©ã‚¦ã‚¶åˆ†å¸ƒ
    with tabs[0]:
        st.subheader(i18n.get("browser_distribution_title") + title_suffix)
        browser_dist = StatisticsAnalyzer.get_browser_distribution(df)
        # é™é †ã§ã‚½ãƒ¼ãƒˆ
        browser_dist: pl.DataFrame = browser_dist.sort("count", descending=True)

        fig: Figure = PlotlyCharts.create_bar_chart(
            browser_dist,
            "Web Browser",
            "count",
            i18n.get("browser_distribution_title") + title_suffix,
            orientation="v",
            show_values=True,
            show_percentage=True,
        )
        st.plotly_chart(fig, width="stretch")

    # ãƒ•ã‚©ãƒ«ãƒ€åˆ†å¸ƒ
    with tabs[1]:
        st.subheader(i18n.get("folder_distribution_title", n="") + title_suffix)

        # è¨­å®šãƒ‘ãƒãƒ«
        col1, col2 = st.columns([3, 1])
        with col2:
            top_n_folder = st.number_input(
                i18n.get("top_n_label"),
                min_value=5,
                max_value=100,
                value=15,
                step=5,
                key="folder_top_n",
            )

        folder_dist: pl.DataFrame = StatisticsAnalyzer.get_folder_distribution(
            df, top_n=top_n_folder
        )
        # é™é †ã§ã‚½ãƒ¼ãƒˆï¼ˆæ¨ªå‘ãæ£’ã‚°ãƒ©ãƒ•ã¯ä¸‹ã‹ã‚‰ä¸Šãªã®ã§ã€æ˜‡é †ã«ã—ã¦ä¸Šä½ãŒä¸Šã«æ¥ã‚‹ã‚ˆã†ã«ã™ã‚‹ï¼‰
        folder_dist = folder_dist.sort("count", descending=False)

        fig = PlotlyCharts.create_bar_chart(
            folder_dist,
            "Folder Name",
            "count",
            i18n.get("folder_distribution_title", n=top_n_folder) + title_suffix,
            orientation="h",
            show_values=True,
            show_percentage=False,
        )
        st.plotly_chart(fig, width="stretch")

    # ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ†å¸ƒ
    with tabs[2]:
        st.subheader(i18n.get("domain_distribution_title", n="") + title_suffix)

        # è¨­å®šãƒ‘ãƒãƒ«
        col1, col2 = st.columns([3, 1])
        with col2:
            top_n_domain = st.number_input(
                i18n.get("top_n_label"),
                min_value=5,
                max_value=100,
                value=15,
                step=5,
                key="domain_top_n",
            )

        domain_dist: pl.DataFrame = StatisticsAnalyzer.get_domain_distribution(
            df, top_n=top_n_domain
        )
        # é™é †ã§ã‚½ãƒ¼ãƒˆï¼ˆæ¨ªå‘ãæ£’ã‚°ãƒ©ãƒ•ã¯ä¸‹ã‹ã‚‰ä¸Šãªã®ã§ã€æ˜‡é †ã«ã—ã¦ä¸Šä½ãŒä¸Šã«æ¥ã‚‹ã‚ˆã†ã«ã™ã‚‹ï¼‰
        domain_dist = domain_dist.sort("count", descending=False)

        fig = PlotlyCharts.create_bar_chart(
            domain_dist,
            "domain",
            "count",
            i18n.get("domain_distribution_title", n=top_n_domain) + title_suffix,
            orientation="h",
            show_values=True,
            show_percentage=True,
        )
        st.plotly_chart(fig, width="stretch")

    # æœˆæ¬¡æ¨ç§»
    with tabs[3]:
        # ã‚µãƒ–ãƒ˜ãƒƒãƒ€ãƒ¼ï¼šå¹´ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨æ™‚ã¯ã€Œæœˆæ¬¡ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ä½œæˆæ¨ç§»ã€ã®ã¿
        if not use_all_years and selected_years:
            st.subheader("æœˆæ¬¡ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ä½œæˆæ¨ç§»" + title_suffix)
        else:
            st.subheader(i18n.get("monthly_trend_title_all") + title_suffix)

        # å¹´é¸æŠï¼ˆãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ï¼‰ - å¹´ãƒ•ã‚£ãƒ«ã‚¿ãŒé©ç”¨ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€Œå…¨æœŸé–“ã€ã‚’é™¤å¤–
        available_years_monthly = TimeSeriesAnalyzer.get_available_years(df)
        if not use_all_years and selected_years:
            # å¹´ãƒ•ã‚£ãƒ«ã‚¿ãŒé©ç”¨ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€Œå…¨æœŸé–“ã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãªã—
            year_options: List[str] = [str(y) for y in available_years_monthly]
        else:
            year_options = [i18n.get("all_years")] + [str(y) for y in available_years_monthly]

        selected_year_str = st.radio(
            i18n.get("year_filter_label"),
            options=year_options,
            horizontal=True,
            key="monthly_year",
        )

        selected_year: Optional[int] = None
        if selected_year_str != i18n.get("all_years"):
            selected_year = int(selected_year_str)

        monthly_counts: pl.DataFrame = TimeSeriesAnalyzer.get_monthly_counts(df, year=selected_year)

        # å¹´-æœˆãƒ©ãƒ™ãƒ«ã‚’ä½œæˆ
        monthly_counts = monthly_counts.with_columns(
            [
                (
                    pl.col("created_year").cast(pl.Utf8)
                    + "-"
                    + pl.col("created_month").cast(pl.Utf8).str.zfill(2)
                ).alias("year_month")
            ]
        )

        # ã‚°ãƒ©ãƒ•ã‚¿ã‚¤ãƒˆãƒ«ï¼šå¹´ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨æ™‚ã¯æ‹¬å¼§éƒ¨åˆ†ã‚’å«ã‚ãªã„
        if not use_all_years and selected_years:
            # å¹´ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨æ™‚ã¯æ‹¬å¼§ãªã—
            title = "æœˆæ¬¡ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ä½œæˆæ¨ç§»"
        else:
            # é€šå¸¸æ™‚
            title: str = (
                i18n.get("monthly_trend_title", year=selected_year)
                if selected_year
                else i18n.get("monthly_trend_title_all")
            )

        fig = PlotlyCharts.create_line_chart(
            monthly_counts,
            "year_month",
            "count",
            title + title_suffix,
            show_markers=True,
            show_values=False,
        )
        st.plotly_chart(fig, width="stretch")

    # å¹´æ¬¡æ¨ç§» - å…¨æœŸé–“é¸æŠæ™‚ã®ã¿è¡¨ç¤º
    if use_all_years:
        with tabs[4]:
            st.subheader(i18n.get("yearly_trend_title"))

            yearly_counts: pl.DataFrame = TimeSeriesAnalyzer.get_yearly_counts(df)

            fig = PlotlyCharts.create_line_chart(
                yearly_counts,
                "created_year",
                "count",
                i18n.get("yearly_trend_title"),
                show_markers=True,
                show_values=True,
            )
            st.plotly_chart(fig, width="stretch")
    else:
        with tabs[4]:
            st.info("å¹´æ¬¡æ¨ç§»ã¯å…¨æœŸé–“ã‚’é¸æŠã—ãŸå ´åˆã®ã¿è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")

    # æ›œæ—¥ãƒ‘ã‚¿ãƒ¼ãƒ³
    with tabs[5]:
        st.subheader(i18n.get("weekday_pattern_title") + title_suffix)

        weekday_dist = TimeSeriesAnalyzer.get_weekday_distribution(df)

        # å¤šè¨€èªå¯¾å¿œã®æ›œæ—¥åã«ç½®ãæ›ãˆ
        weekday_names: List[str] = i18n.get_weekday_names()
        weekday_dist: pl.DataFrame = weekday_dist.with_columns(
            [
                pl.col("created_weekday")
                .map_elements(
                    lambda x: weekday_names[x] if 0 <= x < 7 else "Unknown",
                    return_dtype=pl.Utf8,
                )
                .alias("weekday_name")
            ]
        )

        fig = PlotlyCharts.create_bar_chart(
            weekday_dist,
            "weekday_name",
            "count",
            i18n.get("weekday_pattern_title") + title_suffix,
            show_values=True,
        )
        st.plotly_chart(fig, width="stretch")

    # æ™‚é–“ãƒ‘ã‚¿ãƒ¼ãƒ³
    with tabs[6]:
        st.subheader(i18n.get("hour_pattern_title") + title_suffix)

        hourly_dist = TimeSeriesAnalyzer.get_hourly_distribution(df)

        # ã™ã¹ã¦ã®æ™‚é–“ï¼ˆ0-23ï¼‰ã‚’ç¢ºä¿ã—ã€æ¬ æå€¤ã¯0ã§åŸ‹ã‚ã‚‹
        all_hours = pl.DataFrame({"created_hour": list(range(24))})
        hourly_dist: pl.DataFrame = all_hours.join(
            hourly_dist, on="created_hour", how="left"
        ).fill_null(0)

        fig = PlotlyCharts.create_bar_chart(
            hourly_dist,
            "created_hour",
            "count",
            i18n.get("hour_pattern_title") + title_suffix,
            show_values=True,  # ã™ã¹ã¦ã®æ£’ã«æ•°å€¤ã‚’è¡¨ç¤º
        )
        st.plotly_chart(fig, width="stretch")

    # æ›œæ—¥-æ™‚é–“ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
    with tabs[7]:
        st.subheader(i18n.get("weekday_hour_heatmap_title") + title_suffix)

        heatmap_data: Dict[str, Any] = TimeSeriesAnalyzer.get_weekday_hour_heatmap(df)

        fig = PlotlyCharts.create_heatmap(
            heatmap_data,
            i18n.get("weekday_hour_heatmap_title") + title_suffix,
            colorscale="YlOrRd",
        )
        st.plotly_chart(fig, width="stretch")

    # éšå±¤ãƒ„ãƒªãƒ¼ãƒãƒƒãƒ—
    with tabs[8]:
        st.subheader(i18n.get("hierarchy_treemap_title") + title_suffix)

        # è¨­å®šãƒ‘ãƒãƒ«
        col1, col2 = st.columns([3, 1])

        with col2:
            treemap_mode = st.radio(
                i18n.get("treemap_mode_label"),
                options=[
                    i18n.get("treemap_hierarchical"),
                    i18n.get("treemap_grouped"),
                ],
                key="treemap_mode",
            )

            treemap_height = st.slider(
                i18n.get("height_label"),
                min_value=400,
                max_value=1200,
                value=600,
                step=100,
                key="treemap_height",
            )

        hierarchical: bool = treemap_mode == i18n.get("treemap_hierarchical")

        treemap_data: Dict[str, Any] = HierarchyAnalyzer.build_treemap_data(
            df,
            max_depth=None,
            hierarchical=hierarchical,
        )

        fig = PlotlyCharts.create_treemap(
            treemap_data,
            i18n.get("hierarchy_treemap_title") + title_suffix,
            height=treemap_height,
        )
        st.plotly_chart(fig, width="stretch")

    # ãƒ•ã‚©ãƒ«ãƒ€ãƒ„ãƒªãƒ¼
    with tabs[9]:
        st.subheader(i18n.get("folder_tree_title") + title_suffix)

        tree: Dict[str, Any] = HierarchyAnalyzer.get_hierarchy_tree_structure(df)
        tree_text: str = HierarchyAnalyzer.format_tree_text(tree)

        st.code(tree_text, language=None)

    # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰
    with tabs[10]:
        st.subheader(i18n.get("wordcloud_title") + title_suffix)

        if st.session_state.word_analysis_cache is None:
            st.warning(i18n.get("no_words_found"))
        else:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰è©²å½“ã™ã‚‹å¹´ã®å˜èªé »åº¦ã‚’å–å¾—
            word_freq = _get_word_freq_from_cache(
                st.session_state.word_analysis_cache, use_all_years, selected_years
            )

            if not word_freq:
                st.warning(i18n.get("no_words_found"))
            else:
                # è¨­å®šãƒ‘ãƒãƒ«
                col1, col2 = st.columns([3, 1])

                with col2:
                    # ã‚«ãƒ©ãƒ¼ãƒãƒƒãƒ—é¸æŠ
                    colormap_options = WordCloudGenerator.get_available_colormaps()
                    selected_colormap = st.selectbox(
                        i18n.get("colormap_label"),
                        options=list(colormap_options.keys()),
                        format_func=lambda x: colormap_options[x],
                        index=0,
                        key="wordcloud_colormap",
                    )

                    # æœ€å¤§å˜èªæ•°
                    max_words = st.slider(
                        i18n.get("max_words_label"),
                        min_value=20,
                        max_value=200,
                        value=100,
                        step=10,
                        key="wordcloud_max_words",
                    )

                # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å³åº§ã«ç”Ÿæˆï¼‰
                wc_generator = WordCloudGenerator()
                fig = wc_generator.generate_wordcloud_figure(
                    word_freq,
                    width=1200,
                    height=600,
                    background_color="white",
                    colormap=selected_colormap,
                    max_words=max_words,
                    title=i18n.get("wordcloud_title") + title_suffix,
                )
                st.pyplot(fig)

    # å˜èªãƒ©ãƒ³ã‚­ãƒ³ã‚°
    with tabs[11]:
        st.subheader(i18n.get("word_ranking_title", n="") + title_suffix)

        if st.session_state.word_analysis_cache is None:
            st.warning(i18n.get("no_words_found"))
        else:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰è©²å½“ã™ã‚‹å¹´ã®å˜èªé »åº¦ã‚’å–å¾—
            word_freq = _get_word_freq_from_cache(
                st.session_state.word_analysis_cache, use_all_years, selected_years
            )

            if not word_freq:
                st.warning(i18n.get("no_words_found"))
            else:
                # è¨­å®šãƒ‘ãƒãƒ«
                col1, col2 = st.columns([3, 1])

                with col2:
                    top_n_words = st.number_input(
                        i18n.get("top_n_label"),
                        min_value=5,
                        max_value=200,
                        value=100,
                        step=5,
                        key="word_ranking_top_n",
                    )

                # ä¸Šä½Nä»¶ã‚’å–å¾—
                top_words_df = TextAnalyzer.get_top_words(word_freq, top_n=top_n_words)

                if len(top_words_df) > 0:
                    # é™é †ã§ã‚½ãƒ¼ãƒˆ
                    top_words_df = top_words_df.sort("count", descending=True)

                    # é †ä½ã€ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ã‚’è¿½åŠ 
                    total_count = sum(word_freq.values())
                    top_words_df = top_words_df.with_columns(
                        [
                            pl.Series("rank", list(range(1, len(top_words_df) + 1))),
                            (pl.col("count") / total_count * 100).alias("percentage"),
                        ]
                    )

                    # ã‚«ãƒ©ãƒ ã®é †åºã‚’å¤‰æ›´
                    top_words_df = top_words_df.select(["rank", "word", "count", "percentage"])

                    # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³è¡¨ã¨ã—ã¦è¡¨ç¤º
                    st.markdown("### " + i18n.get("word_ranking_title", n=top_n_words))

                    # DataFrameã‚’Pandasã«å¤‰æ›ã—ã¦è¡¨ç¤º
                    display_df = top_words_df.to_pandas()

                    # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆã‚«ãƒ©ãƒ åå¤‰æ›´å‰ã«å®Ÿè¡Œï¼‰
                    display_df["percentage"] = display_df["percentage"].apply(lambda x: f"{x:.2f}%")

                    # ã‚«ãƒ©ãƒ åã‚’ç¿»è¨³
                    display_df.columns = [
                        i18n.get("rank"),
                        i18n.get("word"),
                        i18n.get("count"),
                        i18n.get("percentage"),
                    ]

                    st.dataframe(display_df, width="stretch", hide_index=True)

                    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
                    st.markdown("---")
                    col_csv, col_json = st.columns(2)

                    # CSV/JSONç”¨ã«ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ã‚’æ•°å€¤ã®ã¾ã¾ä¿æŒã—ãŸDataFrameã‚’ä½œæˆ
                    download_df = top_words_df.with_columns([pl.col("percentage").round(2)])

                    with col_csv:
                        csv_data = download_df.write_csv()
                        st.download_button(
                            label="ğŸ“¥ CSV ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                            data=csv_data,
                            file_name=f"word_ranking_{title_suffix.replace(':', '').replace(' ', '_')}.csv",
                            mime="text/csv",
                        )

                    with col_json:
                        json_data = download_df.write_json()
                        st.download_button(
                            label="ğŸ“¥ JSON ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                            data=json_data,
                            file_name=f"word_ranking_{title_suffix.replace(':', '').replace(' ', '_')}.json",
                            mime="application/json",
                        )
                else:
                    st.warning(i18n.get("no_words_found"))
